{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNJUQ9i9BdAprl7SL7fIRkE",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jmestanza/natural-language-processing-practice/blob/main/desafios/Desafio_3/Desafio_3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Desafío 3\n",
        "Entrenar un modelo de lenguaje basado en caracteres utilizando la [notebook](https://github.com/jmestanza/procesamiento_lenguaje_natural/blob/main/clase_3/ejercicios/3_modelo_lenguaje_char.ipynb) de clase como base .\n",
        "\n",
        "Objetivos principales\n",
        "- Comprender el modelo de lenguaje basado en caracteres.\n",
        "- Definir un corpus para entrenar el modelo.\n",
        "- Tomar decisiones sobre la arquitectura del modelo.\n",
        "- Ejecutar el entrenamiento y analizar los resultados.\n",
        "- Generar secuencias de texto utilizando el modelo entrenado.\n",
        "\n",
        "Descripción de la tarea\n",
        "Se proporciona un notebook base que trabaja con el libro La vuelta al mundo en 80 días.\n",
        "\n",
        "Se realiza preprocesamiento del texto, eliminando etiquetas HTML y convirtiendo todo a minúsculas para reducir el vocabulario.\n",
        "\n",
        "Se define un vocabulario de caracteres, incluyendo letras, signos de puntuación y espacios.\n",
        "\n",
        "Se crean diccionarios de tokenización (carácter a índice y viceversa).\n",
        "\n",
        "Se divide el texto en conjuntos de entrenamiento y validación, tomando el 10% del texto para validación.\n",
        "\n",
        "El modelo se entrena en un esquema many-to-many, donde la entrada es una secuencia de caracteres y la salida es la misma secuencia desplazada en un carácter.\n",
        "\n",
        "Se menciona el uso de capa embedding y time-distributed layers para manejar secuencias.\n",
        "\n",
        "Se mide la perplejidad del modelo como métrica de evaluación.\n",
        "\n",
        "Generación de texto\n",
        "Se propone experimentar con diferentes estrategias de generación de secuencias.\n",
        "\n",
        "Se pueden probar enfoques deterministas o estocásticos para generar texto.\n",
        "\n",
        "Se debe documentar los resultados de generación obtenidos con el modelo final.\n",
        "\n",
        "Entrega esperada\n",
        "Modelo de lenguaje entrenado con la arquitectura seleccionada.\n",
        "\n",
        "Ejemplos de generación de texto con el modelo.\n",
        "\n",
        "Reflexión sobre los resultados y ajustes realizados.\n",
        "\n",
        "En resumen, el trabajo implica entrenar un modelo de lenguaje basado en caracteres, ajustar su arquitectura, medir su rendimiento y generar texto con él."
      ],
      "metadata": {
        "id": "JGWTDDTSXMWD"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "2s4QTbQwXP-m"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}