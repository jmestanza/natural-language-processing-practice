{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jmestanza/natural-language-processing-practice/blob/main/desafios/Desafio_1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Desafío 1 de Procesamiento de Lenguaje Natural\n",
        "## Autor: Mestanza Joaquín\n",
        "## Numero de SIU: a1726"
      ],
      "metadata": {
        "id": "6UsrErfpOP3B"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1XJpt8cSZKpF",
        "outputId": "6d27c3bd-347b-40d3-f422-69fca3fdc884"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (2.0.2)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (1.6.1)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.14.1)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (3.6.0)\n"
          ]
        }
      ],
      "source": [
        "%pip install numpy scikit-learn"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zq6j8LsYq1Dr"
      },
      "source": [
        "### Vectorización de texto y modelo de clasificación Naïve Bayes con el dataset 20 newsgroups"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "l7cXR6CI30ry"
      },
      "outputs": [],
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from sklearn.naive_bayes import MultinomialNB, ComplementNB\n",
        "from sklearn.metrics import f1_score\n",
        "\n",
        "# 20newsgroups por ser un dataset clásico de NLP ya viene incluido y formateado\n",
        "# en sklearn\n",
        "from sklearn.datasets import fetch_20newsgroups\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yD-pVDWV_rQc"
      },
      "source": [
        "## Carga de datos"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "Ech9qJaUo9vK"
      },
      "outputs": [],
      "source": [
        "# cargamos los datos (ya separados de forma predeterminada en train y test)\n",
        "newsgroups_train = fetch_20newsgroups(subset='train', remove=('headers', 'footers', 'quotes'))\n",
        "newsgroups_test = fetch_20newsgroups(subset='test', remove=('headers', 'footers', 'quotes'))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UxjSI7su_uWI"
      },
      "source": [
        "## Vectorización"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "-94VP0QYCzDn"
      },
      "outputs": [],
      "source": [
        "# instanciamos un vectorizador\n",
        "# ver diferentes parámetros de instanciación en la documentación de sklearn https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html\n",
        "tfidfvect = TfidfVectorizer()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ftPlyanuak8n",
        "outputId": "64391b12-9ce8-4137-f0ad-b264608e16e4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "I was wondering if anyone out there could enlighten me on this car I saw\n",
            "the other day. It was a 2-door sports car, looked to be from the late 60s/\n",
            "early 70s. It was called a Bricklin. The doors were really small. In addition,\n",
            "the front bumper was separate from the rest of the body. This is \n",
            "all I know. If anyone can tellme a model name, engine specs, years\n",
            "of production, where this car is made, history, or whatever info you\n",
            "have on this funky looking car, please e-mail.\n"
          ]
        }
      ],
      "source": [
        "# en el atributo `data` accedemos al texto\n",
        "print(newsgroups_train.data[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "1zxcXV6aC_oL"
      },
      "outputs": [],
      "source": [
        "# con la interfaz habitual de sklearn podemos fitear el vectorizador\n",
        "# (obtener el vocabulario y calcular el vector IDF)\n",
        "# y transformar directamente los datos\n",
        "X_train = tfidfvect.fit_transform(newsgroups_train.data)\n",
        "# `X_train` la podemos denominar como la matriz documento-término"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4Sv7TXbda41-",
        "outputId": "212db0f9-1083-4d12-a7e1-a02c82f915b6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'scipy.sparse._csr.csr_matrix'>\n",
            "shape: (11314, 101631)\n",
            "Cantidad de documentos: 11314\n",
            "Tamaño del vocabulario (dimensionalidad de los vectores): 101631\n"
          ]
        }
      ],
      "source": [
        "# recordar que las vectorizaciones por conteos son esparsas\n",
        "# por ello sklearn convenientemente devuelve los vectores de documentos\n",
        "# como matrices esparsas\n",
        "print(type(X_train))\n",
        "print(f'shape: {X_train.shape}')\n",
        "print(f'Cantidad de documentos: {X_train.shape[0]}')\n",
        "print(f'Tamaño del vocabulario (dimensionalidad de los vectores): {X_train.shape[1]}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dgydNTZ2pAgR",
        "outputId": "908d9674-fdd9-4dc9-e932-51ca252a8bde"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "25775"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ],
      "source": [
        "# una vez fiteado el vectorizador, podemos acceder a atributos como el vocabulario\n",
        "# aprendido. Es un diccionario que va de términos a índices.\n",
        "# El índice es la posición en el vector de documento.\n",
        "tfidfvect.vocabulary_['car']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "xnTSZuvyrTcP"
      },
      "outputs": [],
      "source": [
        "# es muy útil tener el diccionario opuesto que va de índices a términos\n",
        "idx2word = {v: k for k,v in tfidfvect.vocabulary_.items()}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "swa-AgWrMSHM",
        "outputId": "8a3b509e-2576-4913-cf22-8aeff211d580"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([ 7,  4,  4,  1, 14, 16, 13,  3,  2,  4])"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ],
      "source": [
        "# en `y_train` guardamos los targets que son enteros\n",
        "y_train = newsgroups_train.target\n",
        "y_train[:10]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "je5kxvQMDLvf",
        "outputId": "edcd3233-1dcf-436a-8908-5bfdbee38a51"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "clases [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['alt.atheism',\n",
              " 'comp.graphics',\n",
              " 'comp.os.ms-windows.misc',\n",
              " 'comp.sys.ibm.pc.hardware',\n",
              " 'comp.sys.mac.hardware',\n",
              " 'comp.windows.x',\n",
              " 'misc.forsale',\n",
              " 'rec.autos',\n",
              " 'rec.motorcycles',\n",
              " 'rec.sport.baseball',\n",
              " 'rec.sport.hockey',\n",
              " 'sci.crypt',\n",
              " 'sci.electronics',\n",
              " 'sci.med',\n",
              " 'sci.space',\n",
              " 'soc.religion.christian',\n",
              " 'talk.politics.guns',\n",
              " 'talk.politics.mideast',\n",
              " 'talk.politics.misc',\n",
              " 'talk.religion.misc']"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ],
      "source": [
        "# hay 20 clases correspondientes a los 20 grupos de noticias\n",
        "print(f'clases {np.unique(newsgroups_test.target)}')\n",
        "newsgroups_test.target_names"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SXCICFSd_y90"
      },
      "source": [
        "## Similaridad de documentos"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_pki_olShnyE",
        "outputId": "dcf2228c-2dcf-4b2e-876c-6308e9ac566f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "THE WHITE HOUSE\n",
            "\n",
            "                  Office of the Press Secretary\n",
            "                   (Pittsburgh, Pennslyvania)\n",
            "______________________________________________________________\n",
            "For Immediate Release                         April 17, 1993     \n",
            "\n",
            "             \n",
            "                  RADIO ADDRESS TO THE NATION \n",
            "                        BY THE PRESIDENT\n",
            "             \n",
            "                Pittsburgh International Airport\n",
            "                    Pittsburgh, Pennsylvania\n",
            "             \n",
            "             \n",
            "10:06 A.M. EDT\n",
            "             \n",
            "             \n",
            "             THE PRESIDENT:  Good morning.  My voice is coming to\n",
            "you this morning through the facilities of the oldest radio\n",
            "station in America, KDKA in Pittsburgh.  I'm visiting the city to\n",
            "meet personally with citizens here to discuss my plans for jobs,\n",
            "health care and the economy.  But I wanted first to do my weekly\n",
            "broadcast with the American people. \n",
            "             \n",
            "             I'm told this station first broadcast in 1920 when\n",
            "it reported that year's presidential elections.  Over the past\n",
            "seven decades presidents have found ways to keep in touch with\n",
            "the people, from whistle-stop tours to fire-side chats to the bus\n",
            "tour that I adopted, along with Vice President Gore, in last\n",
            "year's campaign.\n",
            "             \n",
            "             Every Saturday morning I take this time to talk with\n",
            "you, my fellow Americans, about the problems on your minds and\n",
            "what I'm doing to try and solve them.  It's my way of reporting\n",
            "to you and of giving you a way to hold me accountable.\n",
            "             \n",
            "             You sent me to Washington to get our government and\n",
            "economy moving after years of paralysis and policy and a bad\n",
            "experiment with trickle-down economics.  You know how important\n",
            "it is for us to make bold, comprehensive changes in the way we do\n",
            "business.  \n",
            "             \n",
            "             We live in a competitive global economy.  Nations\n",
            "rise and fall on the skills of their workers, the competitiveness\n",
            "of their companies, the imagination of their industries, and the\n",
            "cooperative experience and spirit that exists between business,\n",
            "labor and government.  Although many of the economies of the\n",
            "industrialized world are now suffering from slow growth, they've\n",
            "made many of the smart investments and the tough choices which\n",
            "our government has for too long ignored.  That's why many of them\n",
            "have been moving ahead and too many of our people have been\n",
            "falling behind.\n",
            "             \n",
            "             We have an economy today that even when it grows is\n",
            "not producing new jobs.  We've increased the debt of our nation\n",
            "by four times over the last 12 years, and we don't have much to\n",
            "show for it.  We know that wages of most working people have\n",
            "stopped rising, that most people are working longer work weeks\n",
            "and that too many families can no longer afford the escalating\n",
            "cost of health care.\n",
            "             \n",
            "             But we also know that, given the right tools, the\n",
            "right incentives and the right encouragement, our workers and\n",
            "businesses can make the kinds of products and profits our economy\n",
            "needs to expand opportunity and to make our communities better\n",
            "places to live.\n",
            "             \n",
            "             In many critical products today Americans are the\n",
            "low cost, high quality producers.  Our task is to make sure that\n",
            "we create more of those kinds of jobs.\n",
            "             \n",
            "             Just two months ago I gave Congress my plan for\n",
            "long-term jobs and economic growth.  It changes the old\n",
            "priorities in Washington and puts our emphasis where it needs to\n",
            "be -- on people's real needs, on increasing investments and jobs\n",
            "and education, on cutting the federal deficit, on stopping the\n",
            "waste which pays no dividends, and redirecting our precious\n",
            "resources toward investment that creates jobs now and lays the\n",
            "groundwork for robust economic growth in the future.\n",
            "             \n",
            "             These new directions passed the Congress in record\n",
            "time and created a new sense of hope and opportunity in our\n",
            "country.  Then the jobs plan I presented to Congress, which would\n",
            "create hundreds of thousands of jobs, most of them in the private\n",
            "sector in 1993 and 1994, passed the House of Representatives.  It\n",
            "now has the support of a majority of the United States Senate. \n",
            "But it's been held up by a filibuster of a minority in the\n",
            "Senate, just 43 senators.  They blocked a vote that they know\n",
            "would result in the passage of our bill and the creation of jobs.\n",
            "             \n",
            "             The issue isn't politics; the issue is people. \n",
            "Millions of Americans are waiting for this legislation and\n",
            "counting on it, counting on us in Washington.  But the jobs bill\n",
            "has been grounded by gridlock.  \n",
            "             \n",
            "             I know the American people are tired of business as\n",
            "usual and politics as usual.  I know they don't want us to spin\n",
            "or wheels.  They want the recovery to get moving.  So I have\n",
            "taken a first step to break this gridlock and gone the extra\n",
            "mile.  Yesterday I offered to cut the size of this plan by 25\n",
            "percent -- from $16 billion to $12 billion.  \n",
            "             \n",
            "             It's not what I'd hoped for.  With 16 million\n",
            "Americans looking for full-time work, I simply can't let the bill\n",
            "languish when I know that even a compromise bill will mean\n",
            "hundreds of thousands of jobs for our people.  The mandate is to\n",
            "act to achieve change and move the country forward.  By taking\n",
            "this initiative in the face of an unrelenting Senate talkathon, I\n",
            "think we can respond to your mandate and achieve a significant\n",
            "portion of our original goals.\n",
            "             \n",
            "             First, we want to keep the programs as much as\n",
            "possible that are needed to generate jobs and meet human needs,\n",
            "including highway and road construction, summer jobs for young\n",
            "people, immunization for children, construction of waste water\n",
            "sites, and aid to small businesses.  We also want to keep funding\n",
            "for extended unemployment compensation benefits, for people who\n",
            "have been unemployed for a long time because the economy isn't\n",
            "creating jobs.\n",
            "             \n",
            "             Second, I've recommended that all the other programs\n",
            "in the bill be cut across-the-board by a little more than 40\n",
            "percent.\n",
            "             \n",
            "             And third, I've recommended a new element in this\n",
            "program to help us immediately start our attempt to fight against\n",
            "crime by providing $200 million for cities and towns to rehire\n",
            "police officers who lost their jobs during the recession and put\n",
            "them back to work protecting our people.  I'm also going to fight\n",
            "for a tough crime bill because the people of this country need it\n",
            "and deserve it.\n",
            "             \n",
            "             Now, the people who are filibustering this bill --\n",
            "the Republican senators -- say they won't vote for it because it\n",
            "increases deficit spending, because there's extra spending this\n",
            "year that hasn't already been approved.  That sounds reasonable,\n",
            "doesn't it?  Here's what they don't say.  This program is more\n",
            "than paid for by budget cuts over my five-year budget, and this\n",
            "budget is well within the spending limits already approved by the\n",
            "Congress this year.\n",
            "             \n",
            "             It's amazing to me that many of these same senators\n",
            "who are filibustering the bill voted during the previous\n",
            "administration for billions of dollars of the same kind of\n",
            "emergency spending, and much of it was not designed to put the\n",
            "American people to work.  \n",
            "             \n",
            "             This is not about deficit spending.  We have offered\n",
            "a plan to cut the deficit.  This is about where your priorities\n",
            "are -- on people or on politics.  \n",
            "             \n",
            "             Keep in mind that our jobs bill is paid for dollar\n",
            "for dollar.  It is paid for by budget cuts.  And it's the\n",
            "soundest investment we can now make for ourselves and our\n",
            "children.  I urge all Americans to take another look at this jobs\n",
            "and investment program; to consider again the benefits for all of\n",
            "us when we've helped make more American partners working to\n",
            "ensure the future of our nation and the strength of our economy.\n",
            "             \n",
            "             You know, if every American who wanted a job had\n",
            "one, we wouldn't have a lot of the other problems we have in this\n",
            "country today.  This bill is not a miracle, it's a modest first\n",
            "step to try to set off a job creation explosion in this country\n",
            "again.  But it's a step we ought to take.  And it is fully paid\n",
            "for over the life of our budget.\n",
            "             \n",
            "             Tell your lawmakers what you think.  Tell them how\n",
            "important the bill is.  If it passes, we'll all be winners.\n",
            "             \n",
            "             Good morning, and thank you for listening.\n"
          ]
        }
      ],
      "source": [
        "# Veamos similaridad de documentos. Tomemos algún documento\n",
        "idx = 4811\n",
        "print(newsgroups_train.data[idx])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "Ssa9bqJ-hA_v"
      },
      "outputs": [],
      "source": [
        "# midamos la similaridad coseno con todos los documentos de train\n",
        "cossim = cosine_similarity(X_train[idx], X_train)[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p_mDA7p3AzcQ",
        "outputId": "e31d8b6e-3089-4958-c123-6485b11179f9"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([1.        , 0.70930477, 0.67474953, ..., 0.        , 0.        ,\n",
              "       0.        ])"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ],
      "source": [
        "# podemos ver los valores de similaridad ordenados de mayor a menos\n",
        "np.sort(cossim)[::-1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0OIhDA1jAryX",
        "outputId": "c8d5e4df-f0b7-489f-94d9-2348229efc26"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([4811, 6635, 4253, ..., 9019, 9016, 8748])"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ],
      "source": [
        "# y a qué documentos corresponden\n",
        "np.argsort(cossim)[::-1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "hP7qLS4ZBLps"
      },
      "outputs": [],
      "source": [
        "# los 5 documentos más similares:\n",
        "mostsim = np.argsort(cossim)[::-1][1:6]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "QdJLHPJACvaj",
        "outputId": "6c514892-517a-4a39-8f19-385f5f3aade2"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'talk.politics.misc'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 18
        }
      ],
      "source": [
        "# el documento original pertenece a la clase:\n",
        "newsgroups_train.target_names[y_train[idx]]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RWy_73epCbFG",
        "outputId": "6a3891db-d74e-4679-f2d9-85a9ea21dda5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "talk.politics.misc\n",
            "talk.politics.misc\n",
            "talk.politics.misc\n",
            "talk.politics.misc\n",
            "talk.politics.misc\n"
          ]
        }
      ],
      "source": [
        "# y los 5 más similares son de las clases:\n",
        "for i in mostsim:\n",
        "  print(newsgroups_train.target_names[y_train[i]])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zRoNnKwhBqzq"
      },
      "source": [
        "### Modelo de clasificación Naïve Bayes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 80
        },
        "id": "TPM0thDaLk0R",
        "outputId": "e1e98f44-9820-4f17-ef84-a58a7131011c"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "MultinomialNB()"
            ],
            "text/html": [
              "<style>#sk-container-id-1 {\n",
              "  /* Definition of color scheme common for light and dark mode */\n",
              "  --sklearn-color-text: #000;\n",
              "  --sklearn-color-text-muted: #666;\n",
              "  --sklearn-color-line: gray;\n",
              "  /* Definition of color scheme for unfitted estimators */\n",
              "  --sklearn-color-unfitted-level-0: #fff5e6;\n",
              "  --sklearn-color-unfitted-level-1: #f6e4d2;\n",
              "  --sklearn-color-unfitted-level-2: #ffe0b3;\n",
              "  --sklearn-color-unfitted-level-3: chocolate;\n",
              "  /* Definition of color scheme for fitted estimators */\n",
              "  --sklearn-color-fitted-level-0: #f0f8ff;\n",
              "  --sklearn-color-fitted-level-1: #d4ebff;\n",
              "  --sklearn-color-fitted-level-2: #b3dbfd;\n",
              "  --sklearn-color-fitted-level-3: cornflowerblue;\n",
              "\n",
              "  /* Specific color for light theme */\n",
              "  --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
              "  --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, white)));\n",
              "  --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
              "  --sklearn-color-icon: #696969;\n",
              "\n",
              "  @media (prefers-color-scheme: dark) {\n",
              "    /* Redefinition of color scheme for dark theme */\n",
              "    --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
              "    --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, #111)));\n",
              "    --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
              "    --sklearn-color-icon: #878787;\n",
              "  }\n",
              "}\n",
              "\n",
              "#sk-container-id-1 {\n",
              "  color: var(--sklearn-color-text);\n",
              "}\n",
              "\n",
              "#sk-container-id-1 pre {\n",
              "  padding: 0;\n",
              "}\n",
              "\n",
              "#sk-container-id-1 input.sk-hidden--visually {\n",
              "  border: 0;\n",
              "  clip: rect(1px 1px 1px 1px);\n",
              "  clip: rect(1px, 1px, 1px, 1px);\n",
              "  height: 1px;\n",
              "  margin: -1px;\n",
              "  overflow: hidden;\n",
              "  padding: 0;\n",
              "  position: absolute;\n",
              "  width: 1px;\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-dashed-wrapped {\n",
              "  border: 1px dashed var(--sklearn-color-line);\n",
              "  margin: 0 0.4em 0.5em 0.4em;\n",
              "  box-sizing: border-box;\n",
              "  padding-bottom: 0.4em;\n",
              "  background-color: var(--sklearn-color-background);\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-container {\n",
              "  /* jupyter's `normalize.less` sets `[hidden] { display: none; }`\n",
              "     but bootstrap.min.css set `[hidden] { display: none !important; }`\n",
              "     so we also need the `!important` here to be able to override the\n",
              "     default hidden behavior on the sphinx rendered scikit-learn.org.\n",
              "     See: https://github.com/scikit-learn/scikit-learn/issues/21755 */\n",
              "  display: inline-block !important;\n",
              "  position: relative;\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-text-repr-fallback {\n",
              "  display: none;\n",
              "}\n",
              "\n",
              "div.sk-parallel-item,\n",
              "div.sk-serial,\n",
              "div.sk-item {\n",
              "  /* draw centered vertical line to link estimators */\n",
              "  background-image: linear-gradient(var(--sklearn-color-text-on-default-background), var(--sklearn-color-text-on-default-background));\n",
              "  background-size: 2px 100%;\n",
              "  background-repeat: no-repeat;\n",
              "  background-position: center center;\n",
              "}\n",
              "\n",
              "/* Parallel-specific style estimator block */\n",
              "\n",
              "#sk-container-id-1 div.sk-parallel-item::after {\n",
              "  content: \"\";\n",
              "  width: 100%;\n",
              "  border-bottom: 2px solid var(--sklearn-color-text-on-default-background);\n",
              "  flex-grow: 1;\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-parallel {\n",
              "  display: flex;\n",
              "  align-items: stretch;\n",
              "  justify-content: center;\n",
              "  background-color: var(--sklearn-color-background);\n",
              "  position: relative;\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-parallel-item {\n",
              "  display: flex;\n",
              "  flex-direction: column;\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-parallel-item:first-child::after {\n",
              "  align-self: flex-end;\n",
              "  width: 50%;\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-parallel-item:last-child::after {\n",
              "  align-self: flex-start;\n",
              "  width: 50%;\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-parallel-item:only-child::after {\n",
              "  width: 0;\n",
              "}\n",
              "\n",
              "/* Serial-specific style estimator block */\n",
              "\n",
              "#sk-container-id-1 div.sk-serial {\n",
              "  display: flex;\n",
              "  flex-direction: column;\n",
              "  align-items: center;\n",
              "  background-color: var(--sklearn-color-background);\n",
              "  padding-right: 1em;\n",
              "  padding-left: 1em;\n",
              "}\n",
              "\n",
              "\n",
              "/* Toggleable style: style used for estimator/Pipeline/ColumnTransformer box that is\n",
              "clickable and can be expanded/collapsed.\n",
              "- Pipeline and ColumnTransformer use this feature and define the default style\n",
              "- Estimators will overwrite some part of the style using the `sk-estimator` class\n",
              "*/\n",
              "\n",
              "/* Pipeline and ColumnTransformer style (default) */\n",
              "\n",
              "#sk-container-id-1 div.sk-toggleable {\n",
              "  /* Default theme specific background. It is overwritten whether we have a\n",
              "  specific estimator or a Pipeline/ColumnTransformer */\n",
              "  background-color: var(--sklearn-color-background);\n",
              "}\n",
              "\n",
              "/* Toggleable label */\n",
              "#sk-container-id-1 label.sk-toggleable__label {\n",
              "  cursor: pointer;\n",
              "  display: flex;\n",
              "  width: 100%;\n",
              "  margin-bottom: 0;\n",
              "  padding: 0.5em;\n",
              "  box-sizing: border-box;\n",
              "  text-align: center;\n",
              "  align-items: start;\n",
              "  justify-content: space-between;\n",
              "  gap: 0.5em;\n",
              "}\n",
              "\n",
              "#sk-container-id-1 label.sk-toggleable__label .caption {\n",
              "  font-size: 0.6rem;\n",
              "  font-weight: lighter;\n",
              "  color: var(--sklearn-color-text-muted);\n",
              "}\n",
              "\n",
              "#sk-container-id-1 label.sk-toggleable__label-arrow:before {\n",
              "  /* Arrow on the left of the label */\n",
              "  content: \"▸\";\n",
              "  float: left;\n",
              "  margin-right: 0.25em;\n",
              "  color: var(--sklearn-color-icon);\n",
              "}\n",
              "\n",
              "#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {\n",
              "  color: var(--sklearn-color-text);\n",
              "}\n",
              "\n",
              "/* Toggleable content - dropdown */\n",
              "\n",
              "#sk-container-id-1 div.sk-toggleable__content {\n",
              "  max-height: 0;\n",
              "  max-width: 0;\n",
              "  overflow: hidden;\n",
              "  text-align: left;\n",
              "  /* unfitted */\n",
              "  background-color: var(--sklearn-color-unfitted-level-0);\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-toggleable__content.fitted {\n",
              "  /* fitted */\n",
              "  background-color: var(--sklearn-color-fitted-level-0);\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-toggleable__content pre {\n",
              "  margin: 0.2em;\n",
              "  border-radius: 0.25em;\n",
              "  color: var(--sklearn-color-text);\n",
              "  /* unfitted */\n",
              "  background-color: var(--sklearn-color-unfitted-level-0);\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-toggleable__content.fitted pre {\n",
              "  /* unfitted */\n",
              "  background-color: var(--sklearn-color-fitted-level-0);\n",
              "}\n",
              "\n",
              "#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {\n",
              "  /* Expand drop-down */\n",
              "  max-height: 200px;\n",
              "  max-width: 100%;\n",
              "  overflow: auto;\n",
              "}\n",
              "\n",
              "#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {\n",
              "  content: \"▾\";\n",
              "}\n",
              "\n",
              "/* Pipeline/ColumnTransformer-specific style */\n",
              "\n",
              "#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
              "  color: var(--sklearn-color-text);\n",
              "  background-color: var(--sklearn-color-unfitted-level-2);\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-label.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
              "  background-color: var(--sklearn-color-fitted-level-2);\n",
              "}\n",
              "\n",
              "/* Estimator-specific style */\n",
              "\n",
              "/* Colorize estimator box */\n",
              "#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
              "  /* unfitted */\n",
              "  background-color: var(--sklearn-color-unfitted-level-2);\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-estimator.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
              "  /* fitted */\n",
              "  background-color: var(--sklearn-color-fitted-level-2);\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-label label.sk-toggleable__label,\n",
              "#sk-container-id-1 div.sk-label label {\n",
              "  /* The background is the default theme color */\n",
              "  color: var(--sklearn-color-text-on-default-background);\n",
              "}\n",
              "\n",
              "/* On hover, darken the color of the background */\n",
              "#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {\n",
              "  color: var(--sklearn-color-text);\n",
              "  background-color: var(--sklearn-color-unfitted-level-2);\n",
              "}\n",
              "\n",
              "/* Label box, darken color on hover, fitted */\n",
              "#sk-container-id-1 div.sk-label.fitted:hover label.sk-toggleable__label.fitted {\n",
              "  color: var(--sklearn-color-text);\n",
              "  background-color: var(--sklearn-color-fitted-level-2);\n",
              "}\n",
              "\n",
              "/* Estimator label */\n",
              "\n",
              "#sk-container-id-1 div.sk-label label {\n",
              "  font-family: monospace;\n",
              "  font-weight: bold;\n",
              "  display: inline-block;\n",
              "  line-height: 1.2em;\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-label-container {\n",
              "  text-align: center;\n",
              "}\n",
              "\n",
              "/* Estimator-specific */\n",
              "#sk-container-id-1 div.sk-estimator {\n",
              "  font-family: monospace;\n",
              "  border: 1px dotted var(--sklearn-color-border-box);\n",
              "  border-radius: 0.25em;\n",
              "  box-sizing: border-box;\n",
              "  margin-bottom: 0.5em;\n",
              "  /* unfitted */\n",
              "  background-color: var(--sklearn-color-unfitted-level-0);\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-estimator.fitted {\n",
              "  /* fitted */\n",
              "  background-color: var(--sklearn-color-fitted-level-0);\n",
              "}\n",
              "\n",
              "/* on hover */\n",
              "#sk-container-id-1 div.sk-estimator:hover {\n",
              "  /* unfitted */\n",
              "  background-color: var(--sklearn-color-unfitted-level-2);\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-estimator.fitted:hover {\n",
              "  /* fitted */\n",
              "  background-color: var(--sklearn-color-fitted-level-2);\n",
              "}\n",
              "\n",
              "/* Specification for estimator info (e.g. \"i\" and \"?\") */\n",
              "\n",
              "/* Common style for \"i\" and \"?\" */\n",
              "\n",
              ".sk-estimator-doc-link,\n",
              "a:link.sk-estimator-doc-link,\n",
              "a:visited.sk-estimator-doc-link {\n",
              "  float: right;\n",
              "  font-size: smaller;\n",
              "  line-height: 1em;\n",
              "  font-family: monospace;\n",
              "  background-color: var(--sklearn-color-background);\n",
              "  border-radius: 1em;\n",
              "  height: 1em;\n",
              "  width: 1em;\n",
              "  text-decoration: none !important;\n",
              "  margin-left: 0.5em;\n",
              "  text-align: center;\n",
              "  /* unfitted */\n",
              "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
              "  color: var(--sklearn-color-unfitted-level-1);\n",
              "}\n",
              "\n",
              ".sk-estimator-doc-link.fitted,\n",
              "a:link.sk-estimator-doc-link.fitted,\n",
              "a:visited.sk-estimator-doc-link.fitted {\n",
              "  /* fitted */\n",
              "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
              "  color: var(--sklearn-color-fitted-level-1);\n",
              "}\n",
              "\n",
              "/* On hover */\n",
              "div.sk-estimator:hover .sk-estimator-doc-link:hover,\n",
              ".sk-estimator-doc-link:hover,\n",
              "div.sk-label-container:hover .sk-estimator-doc-link:hover,\n",
              ".sk-estimator-doc-link:hover {\n",
              "  /* unfitted */\n",
              "  background-color: var(--sklearn-color-unfitted-level-3);\n",
              "  color: var(--sklearn-color-background);\n",
              "  text-decoration: none;\n",
              "}\n",
              "\n",
              "div.sk-estimator.fitted:hover .sk-estimator-doc-link.fitted:hover,\n",
              ".sk-estimator-doc-link.fitted:hover,\n",
              "div.sk-label-container:hover .sk-estimator-doc-link.fitted:hover,\n",
              ".sk-estimator-doc-link.fitted:hover {\n",
              "  /* fitted */\n",
              "  background-color: var(--sklearn-color-fitted-level-3);\n",
              "  color: var(--sklearn-color-background);\n",
              "  text-decoration: none;\n",
              "}\n",
              "\n",
              "/* Span, style for the box shown on hovering the info icon */\n",
              ".sk-estimator-doc-link span {\n",
              "  display: none;\n",
              "  z-index: 9999;\n",
              "  position: relative;\n",
              "  font-weight: normal;\n",
              "  right: .2ex;\n",
              "  padding: .5ex;\n",
              "  margin: .5ex;\n",
              "  width: min-content;\n",
              "  min-width: 20ex;\n",
              "  max-width: 50ex;\n",
              "  color: var(--sklearn-color-text);\n",
              "  box-shadow: 2pt 2pt 4pt #999;\n",
              "  /* unfitted */\n",
              "  background: var(--sklearn-color-unfitted-level-0);\n",
              "  border: .5pt solid var(--sklearn-color-unfitted-level-3);\n",
              "}\n",
              "\n",
              ".sk-estimator-doc-link.fitted span {\n",
              "  /* fitted */\n",
              "  background: var(--sklearn-color-fitted-level-0);\n",
              "  border: var(--sklearn-color-fitted-level-3);\n",
              "}\n",
              "\n",
              ".sk-estimator-doc-link:hover span {\n",
              "  display: block;\n",
              "}\n",
              "\n",
              "/* \"?\"-specific style due to the `<a>` HTML tag */\n",
              "\n",
              "#sk-container-id-1 a.estimator_doc_link {\n",
              "  float: right;\n",
              "  font-size: 1rem;\n",
              "  line-height: 1em;\n",
              "  font-family: monospace;\n",
              "  background-color: var(--sklearn-color-background);\n",
              "  border-radius: 1rem;\n",
              "  height: 1rem;\n",
              "  width: 1rem;\n",
              "  text-decoration: none;\n",
              "  /* unfitted */\n",
              "  color: var(--sklearn-color-unfitted-level-1);\n",
              "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
              "}\n",
              "\n",
              "#sk-container-id-1 a.estimator_doc_link.fitted {\n",
              "  /* fitted */\n",
              "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
              "  color: var(--sklearn-color-fitted-level-1);\n",
              "}\n",
              "\n",
              "/* On hover */\n",
              "#sk-container-id-1 a.estimator_doc_link:hover {\n",
              "  /* unfitted */\n",
              "  background-color: var(--sklearn-color-unfitted-level-3);\n",
              "  color: var(--sklearn-color-background);\n",
              "  text-decoration: none;\n",
              "}\n",
              "\n",
              "#sk-container-id-1 a.estimator_doc_link.fitted:hover {\n",
              "  /* fitted */\n",
              "  background-color: var(--sklearn-color-fitted-level-3);\n",
              "}\n",
              "</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>MultinomialNB()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow\"><div><div>MultinomialNB</div></div><div><a class=\"sk-estimator-doc-link fitted\" rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.6/modules/generated/sklearn.naive_bayes.MultinomialNB.html\">?<span>Documentation for MultinomialNB</span></a><span class=\"sk-estimator-doc-link fitted\">i<span>Fitted</span></span></div></label><div class=\"sk-toggleable__content fitted\"><pre>MultinomialNB()</pre></div> </div></div></div></div>"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ],
      "source": [
        "# es muy fácil instanciar un modelo de clasificación Naïve Bayes y entrenarlo con sklearn\n",
        "clf = MultinomialNB()\n",
        "clf.fit(X_train, y_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "NrQjzM48Mu4T"
      },
      "outputs": [],
      "source": [
        "# con nuestro vectorizador ya fiteado en train, vectorizamos los textos\n",
        "# del conjunto de test\n",
        "X_test = tfidfvect.transform(newsgroups_test.data)\n",
        "y_test = newsgroups_test.target\n",
        "y_pred =  clf.predict(X_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UkGJhetEPdA4",
        "outputId": "a2b0bd47-2060-4310-a0a4-ef3b4bfde2fd"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.5854345727938506"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ],
      "source": [
        "# el F1-score es una metrica adecuada para reportar desempeño de modelos de claificación\n",
        "# es robusta al desbalance de clases. El promediado 'macro' es el promedio de los\n",
        "# F1-score de cada clase. El promedio 'micro' es equivalente a la accuracy que no\n",
        "# es una buena métrica cuando los datasets son desbalanceados\n",
        "f1_score(y_test, y_pred, average='macro')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "McArD4rSDR2K"
      },
      "source": [
        "### Consigna del desafío 1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lJgf6GQIIEH1"
      },
      "source": [
        "**1**. Vectorizar documentos. Tomar 5 documentos al azar y medir similaridad con el resto de los documentos.\n",
        "Estudiar los 5 documentos más similares de cada uno analizar si tiene sentido\n",
        "la similaridad según el contenido del texto y la etiqueta de clasificación.\n",
        "\n",
        "**2**. Entrenar modelos de clasificación Naïve Bayes para maximizar el desempeño de clasificación\n",
        "(f1-score macro) en el conjunto de datos de test. Considerar cambiar parámteros\n",
        "de instanciación del vectorizador y los modelos y probar modelos de Naïve Bayes Multinomial\n",
        "y ComplementNB.\n",
        "\n",
        "**3**. Transponer la matriz documento-término. De esa manera se obtiene una matriz\n",
        "término-documento que puede ser interpretada como una colección de vectorización de palabras.\n",
        "Estudiar ahora similaridad entre palabras tomando 5 palabras y estudiando sus 5 más similares. **La elección de palabras no debe ser al azar para evitar la aparición de términos poco interpretables, elegirlas \"manualmente\"**.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Punto 1"
      ],
      "metadata": {
        "id": "wARG9ZtblM__"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "np.random.seed(42)\n",
        "#tomamos 5 documentos al azar\n",
        "random_idxs = np.random.choice(len(newsgroups_train.data), 5)\n",
        "print(random_idxs)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pf5Sy1smibMB",
        "outputId": "41f12ad3-74d6-4d73-c0aa-ae19b1004375"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[ 7270   860  5390  5191 11284]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install colorama"
      ],
      "metadata": {
        "id": "lpZmYffiBc4t",
        "outputId": "061e166c-1956-4c8f-e3e4-a28e01c47541",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: colorama in /usr/local/lib/python3.11/dist-packages (0.4.6)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import colorama\n",
        "from colorama import Fore"
      ],
      "metadata": {
        "id": "3PUAPBZhBgjF"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in random_idxs:\n",
        "  doc = newsgroups_train.data[i]\n",
        "  doc_vec = X_train[i]\n",
        "  print(f\"{Fore.BLUE} 📁 Document id={i} label: {newsgroups_train.target_names[y_train[i]]}\")\n",
        "  print(f\"{Fore.WHITE}{doc[:500]}\")\n",
        "  cossim = cosine_similarity(doc_vec, X_train)[0]\n",
        "  most_sim = np.argsort(cossim)[::-1][1:6]\n",
        "  for j in most_sim:\n",
        "    print(f\"{Fore.RED}📝 Document id={j} similar to id={i}, with label: {newsgroups_train.target_names[y_train[j]]}\")\n",
        "    print(f'id={j}: {newsgroups_train.target_names[y_train[j]]}')\n",
        "    related_doc = newsgroups_train.data[j]\n",
        "    print(f\"{Fore.WHITE}{related_doc[:500]}\")\n",
        "    print(\"\\n\")\n",
        "  print(\"\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jhTMZwc525xj",
        "outputId": "7a1f9307-1c3a-4fd9-ad52-c03e679949d9"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[34m 📁 Document id=7270 label: rec.sport.hockey\n",
            "\u001b[37m\n",
            "I think Murray has done a great job.  He's picked up Ciccarelli,\n",
            "Sheppard, Ysebaert, Howe, Coffey, and Riendeau (plus some depth players) \n",
            "without giving up anything the Wings needed or any of his top prospects.\n",
            "All of this in three years.  Has anyone done better?\n",
            "\n",
            "The year before he took over, the Wings didn't even make the playoffs.\n",
            "There was about a year and a half during Demers' stint that the Wings\n",
            "did OK, but that was due to Demers' motavational skills and clutch\n",
            "and grab style.  They did\n",
            "\u001b[31m📝 Document id=1880 similar to id=7270, with label: rec.sport.hockey\n",
            "id=1880: rec.sport.hockey\n",
            "\u001b[37m\n",
            "Devallano went earlier and more extensively to the Russian strategy\n",
            "than anyone else...and was the first GM to \"waste\" high draft choices\n",
            "on young Russians...Devallano would still be GM but he succombed to\n",
            "Demers pleading to make the Oates-Federko et al trade...which is the\n",
            "deal that sealed his fate.\n",
            "\n",
            "Murray has made some decent trades...no doubt...but these are more\n",
            "due to the stupidity or cheapness of other teams than brilliance on\n",
            "his part...Washington was too cheap to pay Ciccarelli so they\n",
            "\n",
            "\n",
            "\u001b[31m📝 Document id=1292 similar to id=7270, with label: talk.politics.mideast\n",
            "id=1292: talk.politics.mideast\n",
            "\u001b[37mAccounts of Anti-Armenian Human Right Violations in Azerbaijan #008 Part B\n",
            "                 Prelude to Current Events in Nagorno-Karabakh\n",
            "\n",
            "\t\t\t\t(Part B of #008)\n",
            "\n",
            "      +------------------------------------------------------------------+\n",
            "      |                                                                  |\n",
            "      | \"Oh, yes, I just remembered. While they were raping me they      |\n",
            "      |  repeated quite frequently, \"Let the Armenian women have babies  |\n",
            "      |  for us, Muslim babies, let the\n",
            "\n",
            "\n",
            "\u001b[31m📝 Document id=6437 similar to id=7270, with label: talk.politics.mideast\n",
            "id=6437: talk.politics.mideast\n",
            "\u001b[37mAccounts of Anti-Armenian Human Rights Violations in Azerbaijan #007\n",
            "                 Prelude to Current Events in Nagorno-Karabakh\n",
            "\n",
            "\n",
            " +--------------------------------------------------------------------------+\n",
            " |                                                                          |\n",
            " | They grab Papa, carry him into one room, and Mamma and me into another.  |\n",
            " | They put Mamma on the bed and start undressing her, beating her legs.    |\n",
            " | They start tearing my clothes, right there, in fron\n",
            "\n",
            "\n",
            "\u001b[31m📝 Document id=9623 similar to id=7270, with label: talk.politics.mideast\n",
            "id=9623: talk.politics.mideast\n",
            "\u001b[37mAccounts of Anti-Armenian Human Right Violations in Azerbaijan #012\n",
            "                 Prelude to Current Events in Nagorno-Karabakh\n",
            "\n",
            "        +---------------------------------------------------------+\n",
            "        |                                                         |\n",
            "        |  I saw a naked girl with her hair down. They were       |\n",
            "        |  dragging her. She kept falling because they were       |\n",
            "        |  pushing her and kicking her. She fell down, it was     |\n",
            "        |  muddy there, and \n",
            "\n",
            "\n",
            "\u001b[31m📝 Document id=7286 similar to id=7270, with label: talk.politics.mideast\n",
            "id=7286: talk.politics.mideast\n",
            "\u001b[37mAccounts of Anti-Armenian Human Right Violations in Azerbaijan #008 Part A\n",
            "                 Prelude to Current Events in Nagorno-Karabakh\n",
            "\t\n",
            "\t\t\t\t(Part A of #008)\n",
            "\n",
            "      +------------------------------------------------------------------+\n",
            "      |                                                                  |\n",
            "      | \"Oh, yes, I just remembered. While they were raping me they      |\n",
            "      |  repeated quite frequently, \"Let the Armenian women have babies  |\n",
            "      |  for us, Muslim babies, let th\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\u001b[34m 📁 Document id=860 label: talk.politics.mideast\n",
            "\u001b[37m\n",
            "\n",
            "\n",
            "This is an interesting question to ponder.  Did Brad/Ali's sickness\n",
            "make Ayatollah-style Islam attractive to him or did this new religion \n",
            "that Brad/Ali has formally adopted give him this sickness?\n",
            "\n",
            "\u001b[31m📝 Document id=7714 similar to id=860, with label: comp.windows.x\n",
            "id=7714: comp.windows.x\n",
            "\u001b[37m\n",
            "I tend to use XIconifyWindow to achieve this effect...  Have you tried that?\n",
            "\n",
            "- Brad\n",
            "\n",
            "\n",
            "\u001b[31m📝 Document id=2551 similar to id=860, with label: sci.crypt\n",
            "id=2551: sci.crypt\n",
            "\u001b[37m\n",
            "\n",
            "If Brad's analysis is correct, it may offer an explanation for why the\n",
            "encryption algorithm is being kept secret.  This will prevent competitors\n",
            "from coming out with Clipper-compatible phones which lack the government-\n",
            "installed \"back door.\"  The strategy Brad describes will only work as long\n",
            "as the only way to get compatible phones is to have ones with the government\n",
            "chips.\n",
            "\n",
            "(It would be nice, from the point of view of personal privacy, if Brad\n",
            "turns out to be right.  As long as people still \n",
            "\n",
            "\n",
            "\u001b[31m📝 Document id=6920 similar to id=860, with label: talk.politics.mideast\n",
            "id=6920: talk.politics.mideast\n",
            "\u001b[37m\n",
            "\n",
            "It's hard to beat a car-bomb with a suicidal driver in getting \n",
            "right up to the target before blowing up.  Even booby-traps and\n",
            "radio-controlled bombs under cars are pretty efficient killers.  \n",
            "You have a point.   \n",
            "\n",
            "\n",
            "Is this part of your Islamic value-system?\n",
            "\n",
            "\n",
            "Had Israeli methods been anything like this, then Iraq wouldn've been\n",
            "nuked long ago, entire Arab towns deported and executions performed by\n",
            "the tens of thousands.  The fact is, though, that Israeli methods\n",
            "aren't even 1/10,000th as evi\n",
            "\n",
            "\n",
            "\u001b[31m📝 Document id=5825 similar to id=860, with label: talk.politics.mideast\n",
            "id=5825: talk.politics.mideast\n",
            "\u001b[37mWe really should try to be as understanding as we can for Brad, because it\n",
            "appears killing is all he knows.\n",
            "\n",
            "\n",
            "\u001b[31m📝 Document id=7956 similar to id=860, with label: talk.politics.misc\n",
            "id=7956: talk.politics.misc\n",
            "\u001b[37m\n",
            "Could it be because you're British, Phill, and living in Germany?\n",
            "While the EC working rules are more liberal than what we have in\n",
            "the 1989 US-Canada FTA, there's probably a law about that (having\n",
            "health insurance coverage is a condition of my being down here,\n",
            "for example).\n",
            "\n",
            "You have mentioned this once before, yet both the NY Times profile on\n",
            "the German sickness funds (late Jan.) and pamphlets that my girlfriend\n",
            "gives to her language students from the German consulate both say that\n",
            "it is \"volu\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\u001b[34m 📁 Document id=5390 label: alt.atheism\n",
            "\u001b[37m\n",
            "\n",
            "Specifically, which changes are you talking about?  Are you arguing\n",
            "that the motto is interpreted as offensive by a larger portion of the\n",
            "population now than 40 years ago?\n",
            "\u001b[31m📝 Document id=20 similar to id=5390, with label: alt.atheism\n",
            "id=20: alt.atheism\n",
            "\u001b[37m\n",
            "[...]\n",
            "\n",
            "These don't seem like \"little things\" to me.  At least, they are orders\n",
            "worse than the motto.  Do you think that the motto is a \"little thing\"\n",
            "that will lead to worse things?\n",
            "\n",
            "\n",
            "\u001b[31m📝 Document id=6376 similar to id=5390, with label: alt.atheism\n",
            "id=6376: alt.atheism\n",
            "\u001b[37m\n",
            "\n",
            "Which objective system are you talking about?  What is its goal?\n",
            "Again, which brand of morality are you talking about?\n",
            "\n",
            "\n",
            "\u001b[31m📝 Document id=1137 similar to id=5390, with label: alt.atheism\n",
            "id=1137: alt.atheism\n",
            "\u001b[37m\n",
            "\n",
            "So, we should ban the ammunition?  Why not get rid of the guns?\n",
            "\n",
            "\n",
            "It is worse than others?  The National Anthem?  Should it be changed too?\n",
            "God Bless America?  The list goes on...\n",
            "\n",
            "\n",
            "Then you'd be no better than the people you despise.\n",
            "\n",
            "\n",
            "Oh?\n",
            "\n",
            "\n",
            "An endorsement, or an acknowledgement?  I think gods are things that people\n",
            "are proud of, but I don't think the motto encourages belief.\n",
            "\n",
            "\n",
            "Is it?\n",
            "\n",
            "[...]\n",
            "\n",
            "Would you approve of such a motto?\n",
            "\n",
            "\n",
            "And removing the tool will solve the problem?\n",
            "\n",
            "Or will it increa\n",
            "\n",
            "\n",
            "\u001b[31m📝 Document id=7491 similar to id=5390, with label: alt.atheism\n",
            "id=7491: alt.atheism\n",
            "\u001b[37m\n",
            "Bobby-\n",
            "\n",
            "A few posts ago you said that Lucifer had no free will.  From the above\n",
            "it seems the JW believes the contrary.\n",
            "\n",
            "Are you talking about the same Lucifer?\n",
            "\n",
            "If so, can you suggest an experiment to determine which of you is wrong?\n",
            "\n",
            "Or do you claim that you are both right?\n",
            "\n",
            "\n",
            "\u001b[31m📝 Document id=10779 similar to id=5390, with label: alt.atheism\n",
            "id=10779: alt.atheism\n",
            "\u001b[37m\n",
            "\n",
            "But you haven't taken into the account of propoganda.  Remember, if you\n",
            "asked Germans before WWII if the Jews shoudl be slaughtered, they would\n",
            "probably answer no, but, after the propoganda machine rolled through, at\n",
            "least some were able to tolerate it.\n",
            "\n",
            "You see, it only takes a small group of fanatics to whip up a general\n",
            "frenzy.\n",
            "\n",
            "\n",
            "Well, they haven't managed to outlaw abortion due to the possible objectivity\n",
            "of the courts.  But, they have managed to create quite a few problems for\n",
            "people that\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\u001b[34m 📁 Document id=5191 label: rec.autos\n",
            "\u001b[37m\n",
            "No one should EVER rely on just a magazine to determine what car they\n",
            "buy, I don't care what magazine. Btw, I subscribe to three other \n",
            "auto rags, I just think CU is getting a bum rap by these macho men\n",
            "from hell who think real men should read . . . .\n",
            "\n",
            "Statements like what you said above have no meaning. People keep on\n",
            "saying \"CU is only good for dishwashing detergent\" or as you\n",
            "all they say. \n",
            "\n",
            "If there were as critical of themsevles as they are of CU maybe there\n",
            "would be some real content.\n",
            "\n",
            "jo\n",
            "\u001b[31m📝 Document id=8714 similar to id=5191, with label: soc.religion.christian\n",
            "id=8714: soc.religion.christian\n",
            "\u001b[37mI dreamed that the great judgment morning had dawned,\n",
            "     and the trumpet had blown.\n",
            "I dreamed that the sinners had gathered for judgment\n",
            "     before the white throne.\n",
            "Oh what weeping and wailing as the lost were told of their fate.\n",
            "They cried for the rock and the mountains.\n",
            "They prayed, but their prayers were too late.\n",
            "The soul that had put off salvation, \n",
            "\"Not tonight I'll get saved by and by.\n",
            " No time now to think of ....... religion,\" \n",
            "Alas, he had found time to die.\n",
            "And I saw a Great White\n",
            "\n",
            "\n",
            "\u001b[31m📝 Document id=10779 similar to id=5191, with label: alt.atheism\n",
            "id=10779: alt.atheism\n",
            "\u001b[37m\n",
            "\n",
            "But you haven't taken into the account of propoganda.  Remember, if you\n",
            "asked Germans before WWII if the Jews shoudl be slaughtered, they would\n",
            "probably answer no, but, after the propoganda machine rolled through, at\n",
            "least some were able to tolerate it.\n",
            "\n",
            "You see, it only takes a small group of fanatics to whip up a general\n",
            "frenzy.\n",
            "\n",
            "\n",
            "Well, they haven't managed to outlaw abortion due to the possible objectivity\n",
            "of the courts.  But, they have managed to create quite a few problems for\n",
            "people that\n",
            "\n",
            "\n",
            "\u001b[31m📝 Document id=6437 similar to id=5191, with label: talk.politics.mideast\n",
            "id=6437: talk.politics.mideast\n",
            "\u001b[37mAccounts of Anti-Armenian Human Rights Violations in Azerbaijan #007\n",
            "                 Prelude to Current Events in Nagorno-Karabakh\n",
            "\n",
            "\n",
            " +--------------------------------------------------------------------------+\n",
            " |                                                                          |\n",
            " | They grab Papa, carry him into one room, and Mamma and me into another.  |\n",
            " | They put Mamma on the bed and start undressing her, beating her legs.    |\n",
            " | They start tearing my clothes, right there, in fron\n",
            "\n",
            "\n",
            "\u001b[31m📝 Document id=5988 similar to id=5191, with label: talk.politics.guns\n",
            "id=5988: talk.politics.guns\n",
            "\u001b[37m ^^^^^\n",
            "\n",
            "What an incrediblt sexist remark! Come now, Mike, what ever possessed you to\n",
            "make such a un-PC remark?  I hope all women out there reading this are as\n",
            "incensed as I am. Remember, WOMAN ARE JUST AS GOOD AS MEN!!!! \n",
            "\n",
            "Women stand up for your right to be just as stupid as men. In fact, insist on\n",
            "every oppurtunity to be even more stupid than men! You've got the right, use\n",
            "it!\n",
            "\n",
            "Hey, it's a slow afternoon and I really don't want to get back to that\n",
            "report...;)\n",
            "\n",
            "BTW: mega-smileys for the humor i\n",
            "\n",
            "\n",
            "\u001b[31m📝 Document id=9623 similar to id=5191, with label: talk.politics.mideast\n",
            "id=9623: talk.politics.mideast\n",
            "\u001b[37mAccounts of Anti-Armenian Human Right Violations in Azerbaijan #012\n",
            "                 Prelude to Current Events in Nagorno-Karabakh\n",
            "\n",
            "        +---------------------------------------------------------+\n",
            "        |                                                         |\n",
            "        |  I saw a naked girl with her hair down. They were       |\n",
            "        |  dragging her. She kept falling because they were       |\n",
            "        |  pushing her and kicking her. She fell down, it was     |\n",
            "        |  muddy there, and \n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\u001b[34m 📁 Document id=11284 label: comp.sys.mac.hardware\n",
            "\u001b[37m\n",
            "And they're more like 1024x1024x8 charging & discharging capacitors in a DRAM\n",
            "SIMM =-)\n",
            "\u001b[31m📝 Document id=7793 similar to id=11284, with label: comp.sys.mac.hardware\n",
            "id=7793: comp.sys.mac.hardware\n",
            "\u001b[37mA SIMM is a small PCB with DRAM chips soldered on.\n",
            "\n",
            "--maarten\n",
            "\n",
            "\n",
            "\u001b[31m📝 Document id=4651 similar to id=11284, with label: sci.electronics\n",
            "id=4651: sci.electronics\n",
            "\u001b[37mFor an upcoming project I want to use 4 Megs of DRAM configured as two 2\n",
            "Meg banks of 16 bit data.  I was wondering if anyone out there knows of a\n",
            "DRAM controller which will handle refreshing the data.  It's ok if the\n",
            "controller doesn't handle bank switching - that part is easy.\n",
            "\n",
            "The only controllers I know of are the ones out of the National\n",
            "Semiconductor DRAM Management Handbook (1988 edition) eg. DP8429.  I would\n",
            "like to know if another manufacturer produces one which may be easier to\n",
            "impleme\n",
            "\n",
            "\n",
            "\u001b[31m📝 Document id=9892 similar to id=11284, with label: comp.sys.mac.hardware\n",
            "id=9892: comp.sys.mac.hardware\n",
            "\u001b[37m10). \n",
            "\n",
            "  A 256K DRAM chip is a 256 kilobit chip whereas a 256K SIMM is a 256\n",
            "kilobyte memory module. The SIMM is a PCB with a 30 pin connector edge\n",
            "and on the SIMM are 8 256 kilobit DRAM chips (making the total memory 256\n",
            "KBytes.\n",
            " \n",
            "  You are correct assuming that SIMMs will not fit into a LaserWriter.\n",
            "Apple printers either require 64 pin SIMMs like those in the Mac IIfx or\n",
            "special memory chips. Contact your Apple dealer to find out exactly what\n",
            "kind of chips you need.\n",
            "\n",
            "\n",
            "\u001b[31m📝 Document id=3080 similar to id=11284, with label: comp.sys.ibm.pc.hardware\n",
            "id=3080: comp.sys.ibm.pc.hardware\n",
            "\u001b[37mT8900DIP.TXT - Jeffrey E. Hundstad (j3gum@vax1.mankato.msus.edu)\n",
            "\n",
            "                     Switch Settings on the Trident 8900C\n",
            "\n",
            "----------------------------------\\         /-----------------|\n",
            "               |-----------| |-------------------| |-----------\n",
            "                   VGA Graphics Adapter Layout #1 (8-DRAM)\n",
            "\n",
            "----------------------------------\\         /-----------------|\n",
            "               |-----------| |-------------------| |-----------\n",
            "                VGA Graphics Adapter Layout #2 (2/4/8 - DRA\n",
            "\n",
            "\n",
            "\u001b[31m📝 Document id=1079 similar to id=11284, with label: rec.motorcycles\n",
            "id=1079: rec.motorcycles\n",
            "\u001b[37mSigha.\n",
            " \n",
            " 1) Trying to figure out a way to put a halogen beam on my CB360T... Are \n",
            "there any easy ways to do this (i.e. a \"slip-in\" bulb replacement)?\n",
            " \n",
            " 2) Was told by a guy at the bike shop that my \"not damn near bright enough\" \n",
            "incandescent beam might be caused by a perma-low battery. So I went and \n",
            "picked up this cheapo \"Motorcycle battery and charging system tester\"... \n",
            "Hook it up to the battery, it's got 3 lights on \"Very good charge\"... Start \n",
            "the engine (to test the charging system), and\n",
            "\n",
            "\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "📁 **Document id=7270** label: rec.sport.hockey\n",
        "\n",
        "### 🔗 Relacionados:\n",
        "- 📝 **id=1880** label: rec.sport.hockey  \n",
        "- 📝 **id=1292** label: talk.politics.mideast  \n",
        "- 📝 **id=6437** label: talk.politics.mideast  \n",
        "- 📝 **id=9623** label: talk.politics.mideast  \n",
        "- 📝 **id=7286** label: talk.politics.mideast  \n",
        "\n",
        "---\n",
        "\n",
        "📁 **Document id=860** label: talk.politics.mideast  \n",
        "\n",
        "### 🔗 Relacionados:\n",
        "- 📝 **id=7714** label: comp.windows.x  \n",
        "- 📝 **id=2551** label: sci.crypt  \n",
        "- 📝 **id=6920** label: talk.politics.mideast  \n",
        "- 📝 **id=5825** label: talk.politics.mideast  \n",
        "- 📝 **id=7956** label: talk.politics.misc  \n",
        "\n",
        "---\n",
        "\n",
        "📁 **Document id=5390** label: alt.atheism  \n",
        "\n",
        "### 🔗 Relacionados:\n",
        "- 📝 **id=20** label: alt.atheism  \n",
        "- 📝 **id=6376** label: alt.atheism  \n",
        "- 📝 **id=1137** label: alt.atheism  \n",
        "- 📝 **id=7491** label: alt.atheism  \n",
        "- 📝 **id=10779** label: alt.atheism  \n",
        "\n",
        "---\n",
        "\n",
        "📁 **Document id=5191** label: rec.autos  \n",
        "\n",
        "\n",
        "### 🔗 Relacionados:\n",
        "- 📝 **id=8714** label: soc.religion.christian  \n",
        "- 📝 **id=10779** label: alt.atheism  \n",
        "- 📝 **id=6437** label: talk.politics.mideast  \n",
        "- 📝 **id=5988** label: talk.politics.guns  \n",
        "- 📝 **id=9623** label: talk.politics.mideast  \n",
        "\n",
        "---\n",
        "\n",
        "📁 **Document id=11284** label: comp.sys.mac.hardware  \n",
        "\n",
        "\n",
        "### 🔗 Relacionados:\n",
        "- 📝 **id=7793** label: comp.sys.mac.hardware  \n",
        "- 📝 **id=4651** label: sci.electronics  \n",
        "- 📝 **id=9892** label: comp.sys.mac.hardware  \n",
        "- 📝 **id=3080** label: comp.sys.ibm.pc.hardware  \n",
        "- 📝 **id=1079** label: rec.motorcycles  \n"
      ],
      "metadata": {
        "id": "Y9TckJsLxbph"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Claramente hay categorías que no tienen nada que ver entre sí, por ejemplo en el primer documento en los relacionados se encuentra mideast, que no tiene nada en común con hockey.\n",
        "Como en ese ejemplo, pasa con los otros, es decir que no lo podemos usar para asociación así como así. Para que este tipo de análisis sea correcto hay que agregar un preprocesamiento de los textos, como por ejemplo filtrar las stopwords o filtrar caracteres especiales."
      ],
      "metadata": {
        "id": "I0BM9xso93qm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. Entrenar modelos de clasificación Naïve Bayes para maximizar el desempeño de clasificación (f1-score macro) en el conjunto de datos de test. Considerar cambiar parámteros de instanciación del vectorizador y los modelos y probar modelos de Naïve Bayes Multinomial y ComplementNB."
      ],
      "metadata": {
        "id": "J3BCccoeZV95"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.naive_bayes import ComplementNB\n",
        "import re\n",
        "# config por default:\n",
        "#(input='content',\n",
        "# encoding='utf-8',\n",
        "# decode_error='strict',\n",
        "# strip_accents=None,\n",
        "# lowercase=True,\n",
        "# preprocessor=None,\n",
        "# tokenizer=None,\n",
        "# analyzer='word',\n",
        "# stop_words=None,\n",
        "# token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
        "# ngram_range=(1, 1), max_df=1.0, min_df=1, max_features=None, vocabulary=None, binary=False, dtype=<class 'numpy.float64'>, norm='l2',\n",
        "# use_idf=True, smooth_idf=True, sublinear_tf=False)\n",
        "\n",
        "vect_configs = {\n",
        "    \"cfg_1\": {},\n",
        "    \"cfg_2\": { \"stop_words\": 'english' },\n",
        "    \"cfg_3\": { \"stop_words\": 'english', \"strip_accents\": 'ascii'},\n",
        "    \"cfg_4\": { \"stop_words\": 'english' , \"max_df\": 0.99, \"min_df\": 0.01},\n",
        "    \"cfg_5\": { \"stop_words\": 'english' , \"sublinear_tf\": True},\n",
        "    \"cfg_6\": { \"stop_words\": 'english' , \"max_features\": 10000},\n",
        "    \"cfg_7\": { \"stop_words\": 'english' , \"token_pattern\": r'(?u)\\b\\w+\\b'}, # filtro caracteres especiales\n",
        "}\n",
        "for cfg_name, cfg in vect_configs.items():\n",
        "  print(f'{Fore.BLUE}configuracion={cfg}')\n",
        "  tfidfvect = TfidfVectorizer(**cfg)\n",
        "  X_train = tfidfvect.fit_transform(newsgroups_train.data)\n",
        "  y_train = newsgroups_train.target\n",
        "  # print(type(X_train))\n",
        "  vocab = tfidfvect.get_feature_names_out()\n",
        "  special_char_words = [word for word in vocab if re.search(r'[^a-zA-Z0-9]', word)]\n",
        "  print(special_char_words[:10])\n",
        "  # ['00_', '01_0', '023_', '02_', '02_8v', '02tm_', '03_', '05_', '05_c', '09k_']\n",
        "  # print(f'Words with special characters ({len(special_char_words)} found):')\n",
        "  # print(f'shape: {X_train.shape}')\n",
        "  print(f'{Fore.WHITE}Cantidad de documentos: {X_train.shape[0]}')\n",
        "  print(f'{Fore.WHITE}Tamaño del vocabulario (dimensionalidad de los vectores): {X_train.shape[1]}')\n",
        "  # print(tfidfvect.vocabulary_)\n",
        "  # print(f'{np.unique(newsgroups_test.target)}')\n",
        "  for clf in [MultinomialNB(), ComplementNB()]:\n",
        "    clf.fit(X_train, y_train)\n",
        "    X_test = tfidfvect.transform(newsgroups_test.data)\n",
        "    y_test = newsgroups_test.target\n",
        "    y_pred =  clf.predict(X_test)\n",
        "    print(f'F1-score {cfg_name}: {f1_score(y_test, y_pred, average=\"macro\"):0.03f} with {clf.__class__.__name__}')"
      ],
      "metadata": {
        "id": "u6kvzTBE9176",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3bc89104-6abe-486c-d086-38a3a85b9f87"
      },
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[34mconfiguracion={}\n",
            "['00_', '01_0', '023_', '02_', '02_8v', '02tm_', '03_', '05_', '05_c', '09k_']\n",
            "\u001b[37mCantidad de documentos: 11314\n",
            "\u001b[37mTamaño del vocabulario (dimensionalidad de los vectores): 101631\n",
            "F1-score cfg_1: 0.585 with MultinomialNB\n",
            "F1-score cfg_1: 0.693 with ComplementNB\n",
            "\u001b[34mconfiguracion={'stop_words': 'english'}\n",
            "['00_', '01_0', '023_', '02_', '02_8v', '02tm_', '03_', '05_', '05_c', '09k_']\n",
            "\u001b[37mCantidad de documentos: 11314\n",
            "\u001b[37mTamaño del vocabulario (dimensionalidad de los vectores): 101322\n",
            "F1-score cfg_2: 0.647 with MultinomialNB\n",
            "F1-score cfg_2: 0.694 with ComplementNB\n",
            "\u001b[34mconfiguracion={'stop_words': 'english', 'strip_accents': 'ascii'}\n",
            "['00_', '01_0', '023_', '02_', '02_8v', '02tm_', '03_', '05_', '05_c', '09k_']\n",
            "\u001b[37mCantidad de documentos: 11314\n",
            "\u001b[37mTamaño del vocabulario (dimensionalidad de los vectores): 101320\n",
            "F1-score cfg_3: 0.647 with MultinomialNB\n",
            "F1-score cfg_3: 0.694 with ComplementNB\n",
            "\u001b[34mconfiguracion={'stop_words': 'english', 'max_df': 0.99, 'min_df': 0.01}\n",
            "[]\n",
            "\u001b[37mCantidad de documentos: 11314\n",
            "\u001b[37mTamaño del vocabulario (dimensionalidad de los vectores): 1142\n",
            "F1-score cfg_4: 0.495 with MultinomialNB\n",
            "F1-score cfg_4: 0.470 with ComplementNB\n",
            "\u001b[34mconfiguracion={'stop_words': 'english', 'sublinear_tf': True}\n",
            "['00_', '01_0', '023_', '02_', '02_8v', '02tm_', '03_', '05_', '05_c', '09k_']\n",
            "\u001b[37mCantidad de documentos: 11314\n",
            "\u001b[37mTamaño del vocabulario (dimensionalidad de los vectores): 101322\n",
            "F1-score cfg_5: 0.639 with MultinomialNB\n",
            "F1-score cfg_5: 0.692 with ComplementNB\n",
            "\u001b[34mconfiguracion={'stop_words': 'english', 'max_features': 10000}\n",
            "['2_', '2r_', '36_', '6_', '8c_', '9_', '_7u', '_8', '__', '___']\n",
            "\u001b[37mCantidad de documentos: 11314\n",
            "\u001b[37mTamaño del vocabulario (dimensionalidad de los vectores): 10000\n",
            "F1-score cfg_6: 0.646 with MultinomialNB\n",
            "F1-score cfg_6: 0.666 with ComplementNB\n",
            "\u001b[34mconfiguracion={'stop_words': 'english', 'token_pattern': '(?u)\\\\b\\\\w+\\\\b'}\n",
            "['00_', '01_0', '023_', '02_', '02_8v', '02tm_', '03_', '05_', '05_c', '09k_']\n",
            "\u001b[37mCantidad de documentos: 11314\n",
            "\u001b[37mTamaño del vocabulario (dimensionalidad de los vectores): 101364\n",
            "F1-score cfg_7: 0.645 with MultinomialNB\n",
            "F1-score cfg_7: 0.695 with ComplementNB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "En general\n",
        "- Cambiando la configuración aumentó el f1-score (macro) de MultinomialNB.\n",
        "- ComplementNB parece tener una mejor performance que MultinomiaNB y sin tener que añadir configuraciones (ver primer resultado).\n",
        "- Vemos una mejora cuando realizamos un filtrado del texto. Hemos encontrado que el vocabulario estaba integrado por cosas como:\n",
        "\n",
        "```\n",
        "['00_', '01_0', '023_', '02_', '02_8v', '02tm_', '03_', '05_', '05_c', '09k_']\n",
        "```\n",
        "Lo cual nos indica que hay asociaciones entre letras y numeros que tampoco tienen mucho sentido y refuerzan el hecho de tener que eliminarlas ya que introducen ruido en el vocabulario.\n"
      ],
      "metadata": {
        "id": "B-Nm4k1-wXdl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**3**. Transponer la matriz documento-término. De esa manera se obtiene una matriz\n",
        "término-documento que puede ser interpretada como una colección de vectorización de palabras.\n",
        "Estudiar ahora similaridad entre palabras tomando 5 palabras y estudiando sus 5 más similares. **La elección de palabras no debe ser al azar para evitar la aparición de términos poco interpretables, elegirlas \"manualmente\"**."
      ],
      "metadata": {
        "id": "cgfGP54H5S6J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# mejor config\n",
        "cfg = {'stop_words': 'english', 'token_pattern': '(?u)\\\\b\\\\w+\\\\b'}\n",
        "tfidfvect = TfidfVectorizer(**cfg)\n",
        "# esta es la matriz esparsa\n",
        "X_train = tfidfvect.fit_transform(newsgroups_train.data)"
      ],
      "metadata": {
        "id": "c0rTiRZX5SNm"
      },
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "idx2word = {v: k for k,v in tfidfvect.vocabulary_.items()}\n",
        "#print(idx2word)\n",
        "# elegimos\n",
        "car_idx = [v for k,v in tfidfvect.vocabulary_.items() if k == 'car'][0]\n",
        "sports_idx = [v for k,v in tfidfvect.vocabulary_.items() if k == 'sports'][0]\n",
        "disk_idx = [v for k,v in tfidfvect.vocabulary_.items() if k == 'disk'][0]\n",
        "network_idx = [v for k,v in tfidfvect.vocabulary_.items() if k == 'network'][0]\n",
        "mac_idx = [v for k,v in tfidfvect.vocabulary_.items() if k == 'mac'][0]\n",
        "\n",
        "indexes = [car_idx, sports_idx, disk_idx, network_idx, mac_idx]\n",
        "# print(idx2word[car_idx])"
      ],
      "metadata": {
        "id": "fZ1exsAd8KH6"
      },
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x_train_t = X_train.transpose()\n",
        "# print(x_train_t)"
      ],
      "metadata": {
        "id": "sh2c5dAi692n"
      },
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for idx in indexes:\n",
        "  print(f'Palabra elegida: {idx2word[idx]}')\n",
        "\n",
        "  cossim = cosine_similarity(x_train_t[idx], x_train_t)[0]\n",
        "  most_sim = np.argsort(cossim)[::-1][1:6]\n",
        "  print('similitudes:')\n",
        "  print([idx2word[el] for el in most_sim])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9GItRoSD7189",
        "outputId": "e4fdfb9a-8a60-46b1-9d06-006a49022a26"
      },
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Palabra elegida: car\n",
            "similitudes:\n",
            "['cars', 'criterium', 'dealer', 'civic', 'owner']\n",
            "Palabra elegida: sports\n",
            "similitudes:\n",
            "['wip', 'jockeys', 'rockin', 'pollute', 'admittdly']\n",
            "Palabra elegida: disk\n",
            "similitudes:\n",
            "['hard', 'floppy', 'drive', 'boot', 'hinds']\n",
            "Palabra elegida: network\n",
            "similitudes:\n",
            "['comit', 'beginer', 'qdeck', 'tweak', 'slowdowns']\n",
            "Palabra elegida: mac\n",
            "similitudes:\n",
            "['se', 'tte', 'deluxe', 'macuser', 'quantums']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Si bien en cada categoria hay al menos 1 o 2 palabras relacionadas, hay algunas en las que hay palabras que no tienen relación en principio. Esto es probablemente porque el vocabulario puede mejorar añadiendo más herramientas de preprocesamiento.\n",
        "\n",
        "Buscando en internet se encontró el siguiente [link](https://www.geeksforgeeks.org/text-preprocessing-for-nlp-tasks/#lemmatization-and-stemming) en el cual se habla de técnicas como:\n",
        "- Removing stopwords (NLTK)\n",
        "- Stemming and Lemmatization\n",
        "- Handling Contractions\n",
        "- Spell Checking\n",
        "\n",
        "Así que intentaremos mejorar el vocabulario en base a estas técnicas."
      ],
      "metadata": {
        "id": "0LOA_Agt5D5k"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "xtohwQ9sNTzP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install contractions"
      ],
      "metadata": {
        "id": "sOXNG6llDWP8",
        "outputId": "ba078f9c-56d6-43a9-de10-fb0c5fbce70d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting contractions\n",
            "  Downloading contractions-0.1.73-py2.py3-none-any.whl.metadata (1.2 kB)\n",
            "Collecting textsearch>=0.0.21 (from contractions)\n",
            "  Downloading textsearch-0.0.24-py2.py3-none-any.whl.metadata (1.2 kB)\n",
            "Collecting anyascii (from textsearch>=0.0.21->contractions)\n",
            "  Downloading anyascii-0.3.2-py3-none-any.whl.metadata (1.5 kB)\n",
            "Collecting pyahocorasick (from textsearch>=0.0.21->contractions)\n",
            "  Downloading pyahocorasick-2.1.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (13 kB)\n",
            "Downloading contractions-0.1.73-py2.py3-none-any.whl (8.7 kB)\n",
            "Downloading textsearch-0.0.24-py2.py3-none-any.whl (7.6 kB)\n",
            "Downloading anyascii-0.3.2-py3-none-any.whl (289 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m289.9/289.9 kB\u001b[0m \u001b[31m15.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pyahocorasick-2.1.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (118 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m118.3/118.3 kB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pyahocorasick, anyascii, textsearch, contractions\n",
            "Successfully installed anyascii-0.3.2 contractions-0.1.73 pyahocorasick-2.1.0 textsearch-0.0.24\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -U nltk\n",
        "!python -c \"import nltk; nltk.download('stopwords')\"\n",
        "!pip install symspellpy\n"
      ],
      "metadata": {
        "id": "auTAfABJFo9k",
        "outputId": "fa431d2b-9304-4224-9039-8fc1a6caf88e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.11/dist-packages (3.9.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk) (8.1.8)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from nltk) (2024.11.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from nltk) (4.67.1)\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "Collecting symspellpy\n",
            "  Downloading symspellpy-6.9.0-py3-none-any.whl.metadata (3.9 kB)\n",
            "Collecting editdistpy>=0.1.3 (from symspellpy)\n",
            "  Downloading editdistpy-0.1.5-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.9 kB)\n",
            "Downloading symspellpy-6.9.0-py3-none-any.whl (2.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.6/2.6 MB\u001b[0m \u001b[31m60.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading editdistpy-0.1.5-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (144 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m144.1/144.1 kB\u001b[0m \u001b[31m10.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: editdistpy, symspellpy\n",
            "Successfully installed editdistpy-0.1.5 symspellpy-6.9.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "import re\n",
        "import contractions\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
        "from textblob import TextBlob\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "from symspellpy import SymSpell\n",
        "import pkg_resources\n",
        "\n",
        "# Initialize SymSpell (load a dictionary)\n",
        "sym_spell = SymSpell()\n",
        "dictionary_path = pkg_resources.resource_filename(\"symspellpy\", \"frequency_dictionary_en_82_765.txt\")\n",
        "sym_spell.load_dictionary(dictionary_path, term_index=0, count_index=1)\n",
        "def fast_spell_check(word):\n",
        "    suggestions = sym_spell.lookup(word, verbosity=2, max_edit_distance=2)\n",
        "    return suggestions[0].term if suggestions else word\n",
        "\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "def lemmatize_text(text):\n",
        "    words = word_tokenize(text)\n",
        "    words = [lemmatizer.lemmatize(word) for word in words]\n",
        "    return \" \".join(words)\n",
        "\n",
        "# Download required NLTK resources\n",
        "# nltk.download('stopwords')\n",
        "# nltk.download('wordnet')\n",
        "\n",
        "# Initialize tools\n",
        "stop_words = set(stopwords.words('english'))  # Stopwords from NLTK\n",
        "stemmer = PorterStemmer()\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "# Custom text preprocessor\n",
        "def custom_preprocessor(text):\n",
        "    # 1. Expand contractions (e.g., \"can't\" → \"cannot\")\n",
        "    text = contractions.fix(text)\n",
        "\n",
        "    # 2. Remove special characters (keeping only words and spaces)\n",
        "    # text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
        "    text = re.sub(r\"[^a-zA-Z0-9'-]+\", \" \", text)\n",
        "\n",
        "\n",
        "    # 3. Tokenize words\n",
        "    words = text.split()\n",
        "\n",
        "    # 4. Spell check and correct each word\n",
        "    # words = [fast_spell_check(word) for word in words]\n",
        "      # 4. Efficient spell-checking (skip words with length < 3 to speed up)\n",
        "    words = [fast_spell_check(word) if len(word) > 2 else word for word in words]\n",
        "\n",
        "\n",
        "    # 5. Remove stopwords\n",
        "    words = [word for word in words if word.lower() not in stop_words]\n",
        "\n",
        "    # 6. Apply stemming and lemmatization\n",
        "    # words = [lemmatizer.lemmatize(stemmer.stem(word)) for word in words]\n",
        "    # words = [lemmatizer.lemmatize(word) for word in words]\n",
        "    # words = [stemmer.stem(word) for word in words]\n",
        "    words = [stemmer.stem(lemmatizer.lemmatize(word)) for word in words]\n",
        "\n",
        "    return ' '.join(words)\n",
        "\n",
        "# Define TF-IDF vectorizer with custom preprocessor\n",
        "tfidfvect = TfidfVectorizer(preprocessor=custom_preprocessor)\n",
        "\n",
        "# Transform the training data\n",
        "X_train = tfidfvect.fit_transform(newsgroups_train.data)\n"
      ],
      "metadata": {
        "id": "DY5WkezpCrZi"
      },
      "execution_count": 83,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x_train_t = X_train.transpose()\n",
        "idx2word = {v: k for k,v in tfidfvect.vocabulary_.items()}\n",
        "car_idx = [v for k,v in tfidfvect.vocabulary_.items() if k == 'car'][0]\n",
        "sports_idx = [v for k,v in tfidfvect.vocabulary_.items() if k == 'sport'][0]\n",
        "disk_idx = [v for k,v in tfidfvect.vocabulary_.items() if k == 'disk'][0]\n",
        "network_idx = [v for k,v in tfidfvect.vocabulary_.items() if k == 'network'][0]\n",
        "mac_idx = [v for k,v in tfidfvect.vocabulary_.items() if k == 'mac'][0]\n",
        "\n",
        "indexes = [car_idx, sports_idx, disk_idx, network_idx, mac_idx]\n",
        "for idx in indexes:\n",
        "  print(f'Palabra elegida: {idx2word[idx]}')\n",
        "\n",
        "  cossim = cosine_similarity(x_train_t[idx], x_train_t)[0]\n",
        "  most_sim = np.argsort(cossim)[::-1][1:6]\n",
        "  print('similitudes:')\n",
        "  print([idx2word[el] for el in most_sim])"
      ],
      "metadata": {
        "id": "_CQ8zNnZDT1k",
        "outputId": "4592e96b-dc5d-4c5d-85db-fdf5e524c2e4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 84,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Palabra elegida: car\n",
            "similitudes:\n",
            "['civic', 'dealer', 'mile', 'owner', 'volvo']\n",
            "Palabra elegida: sport\n",
            "similitudes:\n",
            "['wip', 'peninsula', '3000gt', 'trainer', 'pigeonhol']\n",
            "Palabra elegida: disk\n",
            "similitudes:\n",
            "['floppi', 'drive', 'hard', 'memorex', 'boot']\n",
            "Palabra elegida: network\n",
            "similitudes:\n",
            "['dunedin', 'comit', 'etherrout', 'slowdown', 'localtalk']\n",
            "Palabra elegida: mac\n",
            "similitudes:\n",
            "['se', 'delux', 'kwh', 'appl', '5396']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Para algunas palabras se añadieron más palabras relacionadas (como 'mile' en car) y para algunas otras aparecieron otras nuevas (como 'appl' en mac).\n",
        "\n",
        "- Se encontró una limitante de tiempo de ejecución al tratar de corregir las palabras con:\n",
        "```\n",
        " words = [str(TextBlob(word).correct()) for word in words]\n",
        "```\n",
        "y es por eso que se reemplazo con:\n",
        "\n",
        "\n",
        "```\n",
        "words = [fast_spell_check(word) if len(word) > 2 else word for word in words]\n",
        "```\n",
        "\n",
        "- Hay que tener en cuenta que no hay ningún tipo de noción de contexto al obtener estas similitudes, con lo cual el resultado nos dice que las palabras pueden ser similares pero en otro sentido, no al que estamos acostumbrados.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "PwqBF7nNNa_Z"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "eehjwTNRN0jh"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}